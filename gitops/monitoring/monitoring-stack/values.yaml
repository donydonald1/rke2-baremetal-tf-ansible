# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# kube-prometheus-stack (Prometheus, Alertmanager, Grafana)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
kube-prometheus-stack:
  fullnameOverride: "kps"
  crds:
    enabled: false
    ## The CRD upgrade job mitigates the limitation of helm not being able to upgrade CRDs.
    ## The job will apply the CRDs to the cluster before the operator is deployed, using helm hooks.
    ## It deploy a corresponding clusterrole, clusterrolebinding and serviceaccount to apply the CRDs.
    ## This feature is in preview, off by default and may change in the future.
    upgradeJob:
      enabled: false
      forceConflicts: false
  grafana:
    enabled: true

    env:
      GF_SERVER_ROOT_URL: https://grafana.prod.techsecom.io
      GF_SECURITY_COOKIE_SAMESITE: grafana
      TZ: America/Chicago
      GF_DATABASE_TYPE: postgres
      GF_DATABASE_HOST: cnpg-cluster-rw.cnpg-system.svc.cluster.local:5432
      GF_DATABASE_NAME: grafana
      GF_DATABASE_SSL_MODE: disable

    admin:
      existingSecret: grafana-admin-creds
      userKey: username
      passwordKey: password
    plugins:
    - grafana-exploretraces-app

    assertNoLeakedSecrets: false
    initChownData:
      enabled: false

    useStatefulSet: true
    persistence:
      enabled: true
      type: statefulset
      storageClassName: nfs-csi
      accessModes: [ "ReadWriteMany" ]
      size: 5Gi

    service:
      type: NodePort
      port: 80
      nodePort: 32000
    grafana.ini:
      server:
        root_url: https://grafana.prod.techsecom.io
        # analytics:
        # check_for_updates: false
        # check_for_plugin_updates: true
      auth.anonymous:
        org_name: "Techsecom Consulting Group"
      dataproxy:
        max_idle_connections: 500
      security:
        allow_embedding: true
      feature_toggles:
        provisioning: true
        kubernetesDashboards: true
      auth:
        auto_login: true
        disable_signout_menu: false
      auth.generic_oauth:
        enabled: true
        client_id: <path:secret/data/grafana#client_id>
        client_secret: <path:secret/data/grafana#client_secret>
        allow_sign_up: true
        scopes: openid profile email roles
        auth_url: <path:secret/data/grafana#auth_url>
        token_url: <path:secret/data/grafana#token_url>
        api_url: <path:secret/data/grafana#api_url>
        use_pkce: true
        use_refresh_token: true
      users:
        auto_assign_org: true
        auto_assign_org_role: Editor
      # paths:
      #   data: /var/lib/grafana/data
      #   logs: /var/log/grafana
      #   plugins: /var/lib/grafana/plugins
      #   provisioning: /etc/grafana/provisioning
      database:
        type: postgres
        host: "cnpg-cluster-rw.cnpg-system.svc.cluster.local:5432"
        name: grafana
        user: $__file{/etc/secrets/username}
        password: $__file{/etc/secrets/password}
        ssl_mode: disable
      plugins:
        allow_loading_unsigned_plugins: grafana-pyroscope-app,grafana-exploretraces-app

    extraSecretMounts:
    - name: grafana-db-secret
      secretName: grafana-user-secret
      mountPath: /etc/secrets
      readOnly: true
    serviceMonitor:
      enabled: true
    serviceAccount:
      autoMount: true
    extraEnvFrom:
    - secretRef:
        name: grafana-user-secret
    extraEnv:
    - name: GF_DATABASE_USER
      valueFrom:
        secretKeyRef:
          name: grafana-user-secret
          key: username
    - name: GF_DATABASE_PASSWORD__FILE
      value: /etc/secrets/password

    ingress:
      enabled: true
      ingressClassName: nginx
      annotations:
        kubernetes.io/ingress.class: "nginx"
        cert-manager.io/cluster-issuer: "letsencrypt-prod"
        cert-manager.io/revision-history-limit: "3"
        external-dns.alpha.kubernetes.io/enabled: "true"
        cert-manager.io/duration: "2160h"
        cert-manager.io/renew-before: "720h"

      hosts: [ grafana.prod.techsecom.io ]
      tls:
      - hosts:
        - grafana.prod.techsecom.io
        secretName: grafana-cert-tls
    # Ensure Grafana loads provisioning files
    sidecar:
      datasources:
        enabled: true

    # Strong, deterministic provisioning with stable UIDs
    datasources:
      datasources.yaml:
        apiVersion: 1

        # Cleanly replace any existing entries with same name/uid
        deleteDatasources:
        - name: Thanos
          uid: thanos
        - name: Loki
          uid: loki
        - name: Tempo
          uid: tempo

        datasources:
        # --- Thanos (Prometheus) ---
        - name: Thanos
          uid: thanos
          type: prometheus
          access: proxy
          url: http://monitoring-stack-thanos-query.monitoring.svc.cluster.local:10902
          isDefault: false
          jsonData:
            httpMethod: POST
            # optional: let metrics exemplars link to Tempo traces
            exemplarTraceIdDestinations:
            - datasourceUid: tempo
              name: Tempo
          editable: false

        # --- Loki (Logs) ---
        - name: Loki
          uid: loki
          type: loki
          access: proxy
          url: http://monitoring-stack-loki-gateway.monitoring.svc.cluster.local
          isDefault: false
          secureJsonData:
            httpHeaderValue1: "application/json"
          jsonData:
            httpHeaderName1: "X-scope-OrgID"
            # Link logs â†’ traces (supports either trace_id or traceId fields)
            derivedFields:
            - name: trace_id
              matcherRegex: '"trace_id"\\s*:\\s*"([a-fA-F0-9\\-]+)"'
              datasourceUid: tempo
              internalLink: true
            - name: traceId
              matcherRegex: '"traceId"\\s*:\\s*"([a-fA-F0-9\\-]+)"'
              datasourceUid: tempo
              internalLink: true
          editable: false

        # --- Tempo (Traces) ---
        - name: Tempo
          uid: tempo
          type: tempo
          access: proxy
          url: http://tempo.monitoring.svc.cluster.local:3200
          isDefault: false
          jsonData:
            httpMethod: GET
            tracesToLogsV2:
              datasourceUid: loki
            search:
              hide: false
            lokiSearch:
              datasourceUid: loki
            # Traces â†’ Logs (pivot from a span to relevant logs)
            tracesToLogs:
              datasourceUid: loki
              mapTagNamesEnabled: true
              tags: [ "namespace", "pod", "container", "audit", "app" ]
              filterByTraceID: true
              filterBySpanID: true
            # Service graph uses your metrics source
            serviceMap:
              datasourceUid: thanos
            nodeGraph:
              enabled: true
          editable: false

  kubeProxy:
    enabled: false
  kubeScheduler:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeControllerManager:
    enabled: false
  kubeDns:
    enabled: false
  # kubeRbacProxy:
  #   enabled: false
  # kubeAggregatedApiServer:
  #   enabled: false
  prometheus:
    thanosService:
      enabled: true # exposes gRPC (10901) / HTTP (10902) for the sidecar

    # EXPOSE PROMETHEUS VIA NODEPORT
    service:
      type: NodePort
      port: 9090
      targetPort: 9090
      nodePort: 32090
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    prometheusSpec:
      externalLabels:
        cluster: "ops-dev-dc-bm-rke2"
      thanos:
        image: "quay.io/thanos/thanos:v0.39.2"
        version: "v0.39.2"
      enableFeatures:
      - auto-gomemlimit
      - memory-snapshot-on-shutdown
      - new-service-discovery-manager
      - exemplar-storage
      - native-histograms
      # Ensure Prometheus will pick up *all* ServiceMonitors/PodMonitors
      serviceMonitorSelectorNilUsesHelmValues: true
      podMonitorSelectorNilUsesHelmValues: true
      enableRemoteWriteReceiver: true
      enableAdminAPI: true
      replicas: 1
      # walCompression: true
      # allow operator to pick up ScrapeConfig CRDs from anywhere
      scrapeConfigSelector: {}
      scrapeConfigNamespaceSelector: {}

      resources:
        # requests:
        #   cpu: 1000m
        limits:
          memory: 4Gi
          cpu: 800m

      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: nfs-csi
            resources:
              requests:
                storage: 60Gi
      ingress:
        enabled: true
        pathType: Prefix
        ingressClassName: nginx
        annotations:
          kubernetes.io/ingress.class: "nginx"
          cert-manager.io/cluster-issuer: letsencrypt-prod
          cert-manager.io/revision-history-limit: "3"
          external-dns.alpha.kubernetes.io/enabled: "true"
          cert-manager.io/duration: "2160h"
          cert-manager.io/renew-before: "720h"

        hosts: [ prometheus.prod.techsecom.io ]
        tls:
        - hosts:
          - prometheus.prod.techsecom.io
          secretName: prom-server-cert-tls
      additionalScrapeConfigs:
      - job_name: prometheus
        static_configs:
        - targets:
          - localhost:9090
      - job_name: kubernetes-apiservers
        kubernetes_sd_configs:
        - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [ __meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name ]
          action: keep
          regex: default;kubernetes;https
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [ __meta_kubernetes_pod_annotation_gitlab_com_prometheus_scrape ]
          action: keep
          regex: true
        - source_labels: [ __meta_kubernetes_pod_annotation_gitlab_com_prometheus_scheme ]
          action: replace
          regex: (https?)
          target_label: __scheme__
        - source_labels: [ __meta_kubernetes_pod_annotation_gitlab_com_prometheus_path ]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [ __address__, __meta_kubernetes_pod_annotation_gitlab_com_prometheus_port ]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [ __meta_kubernetes_namespace ]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [ __meta_kubernetes_pod_name ]
          action: replace
          target_label: kubernetes_pod_name
      - job_name: kubernetes-service-endpoints
        kubernetes_sd_configs:
        - role: endpoints
        relabel_configs:
        - action: keep
          regex: true
          source_labels:
          - __meta_kubernetes_service_annotation_gitlab_com_prometheus_scrape
        - action: replace
          regex: (https?)
          source_labels:
          - __meta_kubernetes_service_annotation_gitlab_com_prometheus_scheme
          target_label: __scheme__
        - action: replace
          regex: (.+)
          source_labels:
          - __meta_kubernetes_service_annotation_gitlab_com_prometheus_path
          target_label: __metrics_path__
        - action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          source_labels:
          - __address__
          - __meta_kubernetes_service_annotation_gitlab_com_prometheus_port
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - action: replace
          source_labels:
          - __meta_kubernetes_namespace
          target_label: kubernetes_namespace
        - action: replace
          source_labels:
          - __meta_kubernetes_service_name
          target_label: kubernetes_name
        - action: replace
          source_labels:
          - __meta_kubernetes_pod_node_name
          target_label: kubernetes_node
      - job_name: kubernetes-services
        metrics_path: /probe
        params:
          module: [ http_2xx ]
        kubernetes_sd_configs:
        - role: service
        relabel_configs:
        - source_labels: [ __meta_kubernetes_service_annotation_gitlab_com_prometheus_probe ]
          action: keep
          regex: true
        - source_labels: [ __address__ ]
          target_label: __param_target
        - target_label: __address__
          replacement: blackbox
        - source_labels: [ __param_target ]
          target_label: instance
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [ __meta_kubernetes_namespace ]
          target_label: kubernetes_namespace
        - source_labels: [ __meta_kubernetes_service_name ]
          target_label: kubernetes_name

  alertmanager:
    enabled: true
    config:
      global:
        resolve_timeout: 5m
      route:
        group_by: [ 'alertname', 'job' ]
        group_wait: 45s
        group_interval: 10m
        repeat_interval: 12h
        receiver: "msteams"
        routes:
        - receiver: "null" # quote
          matchers:
          - alertname =~ "Watchdog"
        - receiver: "null" # quote
          matchers:
          - alertname =~ "InfoInhibitor"
        - receiver: "msteams"
          match:
            severity: critical
          continue: true
        - receiver: "msteams"
      inhibit_rules:
      - source_matchers:
        - severity = "critical"
        target_matchers:
        - severity = "warning"
        equal: [ "alertname", "namespace" ]
      - target_match_re:
          alertname: '.+Overcommit'
        source_match:
          alertname: 'Watchdog'
        equal: [ 'prometheus' ]
      receivers:
      - name: "null"
      - name: "msteams"
        msteams_configs:
        - send_resolved: true
          webhook_url: <path:secret/data/msteams#webhook-url>
          title: |-
            ðŸš¨ [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] 
            (Cluster: {{ .CommonLabels.cluster }})

            **ðŸ“¢ Alert Notification - Techsecoms Monitoring**

            {{ if eq .Status "firing" }}ðŸ”¥ **Action Required Immediately!** ðŸ”¥{{ else }}âœ… **Issue Resolved** âœ…{{ end }}

          text: |-
            **ðŸ”” Alert Details:**
            {{ range $index, $alert := .Alerts -}}{{ if $index }}---{{ end }}
            {{ if $alert.Labels.alertname }}
            **ðŸ†˜ Alert Name**: {{ $alert.Labels.alertname }}
            {{ end }}

            **ðŸ›‘ Labels:**
            {{ if $alert.Labels.severity }}
            - **Severity**: {{ $alert.Labels.severity }}
            {{ end }}
            {{ if $alert.Labels.instance }}
            - **Instance**: {{ $alert.Labels.instance }}
            {{ end }}
            {{ if $alert.Labels.namespace }}
            - **Namespace**: {{ $alert.Labels.namespace }}
            {{ end }}
            {{ if $alert.Labels.pod }}
            - **Pod**: {{ $alert.Labels.pod }}
            {{ end }}

            **ðŸ“„ Annotations:**
            {{ if $alert.Annotations.description }}
            - **Description**: {{ $alert.Annotations.description }}
            {{ end }}
            {{ if $alert.Annotations.summary }}
            - **Summary**: {{ $alert.Annotations.summary }}
            {{ end }}

            {{ if $alert.GeneratorURL }}
            ðŸ”— **Source**: [View in AlertManager]({{ $alert.GeneratorURL }})
            {{ end }}

            **ðŸ“Œ Additional Notes:**
            - Ensure the affected service is reviewed immediately.
            - Escalate to the **on-call team** if required.
            - Check related metrics and logs for further debugging.

            {{ end }}
    alertmanagerSpec:
      replicas: 1
      # podAntiAffinity: hard

      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: nfs-csi
            resources:
              requests:
                storage: 10Gi
      tolerations:
      - key: "arm64"
        operator: "Exists"
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Bitnami Thanos (Query only)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
thanos:
  image:
    registry: docker.io
    repository: bitnamilegacy/thanos
    # tag: 0.39.2-debian-12-r2
    digest: ""
  query:
    enabled: true
    replicaCount: 2
    replicaLabels:
    - replica
    dnsDiscovery:
      sidecarsService: kps-thanos-discovery
      sidecarsNamespace: monitoring
    ingress:
      enabled: true
      hostname: thanos.prod.techsecom.io
      annotations:
        gethomepage.dev/enabled: 'true'
        gethomepage.dev/group: Monitoring
        gethomepage.dev/icon: prometheus
        gethomepage.dev/name: Thanos
        kubernetes.io/ingress.class: "nginx"
        cert-manager.io/cluster-issuer: "letsencrypt-prod"
        cert-manager.io/revision-history-limit: "3"
        external-dns.alpha.kubernetes.io/enabled: "true"
        cert-manager.io/duration: "2160h"
        cert-manager.io/renew-before: "720h"
        # nginx.ingress.kubernetes.io/auth-url: "http://oauth2-proxy.kube-system.svc.cluster.local:80/oauth2/auth"
        # nginx.ingress.kubernetes.io/auth-signin: "https://auth.rsr.net/oauth2/start"
      tls: true
  queryFrontend:
    enabled: true
  bucketweb:
    enabled: true
    ingress:
      enabled: true
      annotations:
        kubernetes.io/ingress.class: "nginx"
        cert-manager.io/cluster-issuer: letsencrypt-prod
        cert-manager.io/revision-history-limit: "3"
        external-dns.alpha.kubernetes.io/enabled: "true"
        cert-manager.io/duration: "2160h"
        cert-manager.io/renew-before: "720h"
        # nginx.ingress.kubernetes.io/auth-url: "http://oauth2-proxy.kube-system.svc.cluster.local:80/oauth2/auth"
        # nginx.ingress.kubernetes.io/auth-signin: "https://auth.rsr.net/oauth2/start"
      hostname: thanos-bucketweb.prod.techsecom.io
      tls: true
  compactor:
    enabled: true
    # extraFlags: [ "--compact.concurrency", "4" ]
    retentionResolutionRaw: 7d
    retentionResolution5m: 14d
    retentionResolution1h: 30d
    ingress:
      enabled: true
      annotations:
        kubernetes.io/ingress.class: "nginx"
        cert-manager.io/cluster-issuer: letsencrypt-prod
        cert-manager.io/revision-history-limit: "3"
        external-dns.alpha.kubernetes.io/enabled: "true"
        cert-manager.io/duration: "2160h"
        cert-manager.io/renew-before: "720h"
        # nginx.ingress.kubernetes.io/auth-url: "http://oauth2-proxy.kube-system.svc.cluster.local:80/oauth2/auth"
        # nginx.ingress.kubernetes.io/auth-signin: "https://auth.rsr.net/oauth2/start"
      hostname: thanos-compactor.prod.techsecom.io
      tls: true
    persistence:
      enabled: true
      storageClass: nfs-csi
      size: 20Gi
  storegateway:
    enabled: true
    ingress:
      enabled: true
      annotations:
        kubernetes.io/ingress.class: "nginx"
        cert-manager.io/cluster-issuer: letsencrypt-prod
        cert-manager.io/revision-history-limit: "3"
        external-dns.alpha.kubernetes.io/enabled: "true"
        cert-manager.io/duration: "2160h"
        cert-manager.io/renew-before: "720h"
        # nginx.ingress.kubernetes.io/auth-url: "http://oauth2-proxy.kube-system.svc.cluster.local:80/oauth2/auth"
        # nginx.ingress.kubernetes.io/auth-signin: "https://auth.rsr.net/oauth2/start"
      hostname: thanos-storegateway.prod.techsecom.io
      tls: true
    persistence:
      enabled: true
      storageClass: nfs-csi
      size: 7Gi
  ruler:
    enabled: false
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
  # receive:
  #   ## @param receive.enabled Enable/disable Thanos Receive component
  #   ##
  #   enabled: true
  #   ## @param receive.mode Mode to run receiver in. Valid options are "standalone" or "dual-mode"
  #   ## ref: https://github.com/thanos-io/thanos/blob/release-0.22/docs/proposals-accepted/202012-receive-split.md
  #   ## Enables running the Thanos Receiver in dual mode. Setting this to "dual-mode" will create a deployment for
  #   ## the stateless thanos distributor.
  #   mode: standalone
  #   ## @param receive.logLevel Thanos Receive log level
  #   ##
  #   logLevel: info
  #   resources:
  #     limits:
  #       memory: 1000Mi
  objstoreConfig: |
    type: s3
    config:
      bucket: thanos
      endpoint: "minio.minio.svc.cluster.local:9000"
      access_key: <path:secret/data/minio#postgres_user>
      secret_key: <path:secret/data/minio#postgres_password>
      insecure: true
##############################################################
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Loki 
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
loki:
  deploymentMode: SimpleScalable

  loki:
    auth_enabled: false

    analytics:
      reporting_enabled: false
    compactor:
      working_directory: /var/loki/compactor/retention
      delete_request_store: s3
      retention_enabled: true

    frontend:
      max_outstanding_per_tenant: 4096

    ingester:
      chunk_encoding: snappy

    commonConfig:
      replication_factor: 2

    limits_config:
      ingestion_burst_size_mb: 128
      ingestion_rate_mb: 64
      max_query_parallelism: 100
      per_stream_rate_limit: 64M
      per_stream_rate_limit_burst: 128M
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      retention_period: 14d
      shard_streams:
        enabled: true
      split_queries_by_interval: 1h

    query_scheduler:
      max_outstanding_requests_per_tenant: 4096

    rulerConfig:
      enable_api: true
      enable_alertmanager_v2: true
      alertmanager_url: http://kps-alertmanager.monitoring.svc.cluster.local:9093
      storage:
        type: local
        local:
          directory: /rules
      rule_path: /rules/fake

    schemaConfig:
      configs:
      - from: "2024-04-01"
        store: tsdb
        object_store: s3
        schema: v13
        index:
          prefix: loki_index_
          period: 24h

    server:
      log_level: info
      grpc_server_max_recv_msg_size: 8388608
      grpc_server_max_send_msg_size: 8388608

    storage:
      type: s3
      bucketNames:
        chunks: loki-chunks
        ruler: loki-ruler
        admin: loki-admin
      s3:
        s3ForcePathStyle: true
        insecure: true
        endpoint: http://minio.minio.svc.cluster.local:9000
        secretAccessKey: <path:secret/data/minio#postgres_password>
        accessKeyId: <path:secret/data/minio#postgres_user>
        region: us-east-1

  gateway:
    replicas: 2
    enabled: true
    image:
      registry: docker.io
      repository: nginxinc/nginx-unprivileged
      tag: 1.29-alpine
    nginxConfig:
      resolver: "rke2-coredns-rke2-coredns.kube-system.svc.cluster.local valid=30s ipv6=off"
      serverSnippet: |
        location ^~ /logging/ {
          # Permanent redirect that preserves method (POST) â€” Promtail may/may not follow;
          # prefer fixing Promtail instead of relying on redirects.
          return 308 /loki$uri$is_args$args;
        }
    deploymentStrategy:
      type: RollingUpdate
      rollingUpdate:
        maxUnavailable: 50%
        maxSurge: 50%
    topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
      labelSelector:
        matchLabels:
          app.kubernetes.io/name: loki
          app.kubernetes.io/component: gateway
    ingress:
      enabled: false
      ingressClassName: "nginx"
      annotations:
        cert-manager.io/cluster-issuer: "letsencrypt-prod"
        cert-manager.io/revision-history-limit: "3"
        external-dns.alpha.kubernetes.io/enabled: "true"
        cert-manager.io/duration: "2160h"
        cert-manager.io/renew-before: "720h"
      hosts:
      - host: "loki.prod.techsecom.io"
      tls:
      - secretName: loki-gateway-tls
        hosts:
        - "loki.prod.techsecom.io"

  write:
    replicas: 3
    persistence:
      size: 10Gi
      storageClass: "nfs-csi"
    tolerations:
    - key: "arm64"
      operator: "Exists"
  read:
    replicas: 2

  backend:
    replicas: 2
    persistence:
      storageClass: "nfs-csi"
    tolerations:
    - key: "arm64"
      operator: "Exists"

  ingress:
    enabled: true
    ingressClassName: "nginx"
    annotations:
      cert-manager.io/cluster-issuer: "letsencrypt-prod"
      cert-manager.io/revision-history-limit: "3"
      external-dns.alpha.kubernetes.io/enabled: "true"
      cert-manager.io/duration: "2160h"
      cert-manager.io/renew-before: "720h"
    #    nginx.ingress.kubernetes.io/auth-type: basic
    #    nginx.ingress.kubernetes.io/auth-secret: loki-distributed-basic-auth
    #    nginx.ingress.kubernetes.io/auth-secret-type: auth-map
    #    nginx.ingress.kubernetes.io/configuration-snippet: |
    #      proxy_set_header X-Scope-OrgID $remote_user;
    labels: {}
    #    blackbox.monitoring.exclude: "true"
    paths:
      distributor:
      - /api/prom/push
      - /loki/api/v1/push
      - /otlp/v1/logs
      - /ui
      queryFrontend:
      - /api/prom/query
      - /api/prom/label
      - /api/prom/series
      - /api/prom/tail
      - /loki/api/v1/query
      - /loki/api/v1/query_range
      - /loki/api/v1/tail
      - /loki/api/v1/label
      - /loki/api/v1/labels
      - /loki/api/v1/series
      - /loki/api/v1/index/stats
      - /loki/api/v1/index/volume
      - /loki/api/v1/index/volume_range
      - /loki/api/v1/format_query
      - /loki/api/v1/detected_field
      - /loki/api/v1/detected_fields
      - /loki/api/v1/detected_labels
      - /loki/api/v1/patterns
      ruler:
      - /api/prom/rules
      - /api/prom/api/v1/rules
      - /api/prom/api/v1/alerts
      - /loki/api/v1/rules
      - /prometheus/api/v1/rules
      - /prometheus/api/v1/alerts
      compactor:
      - /loki/api/v1/delete

    hosts:
    - loki.prod.techsecom.io
    tls:
    - secretName: loki-distributed-tls
      hosts:
      - loki.prod.techsecom.io

  #    - hosts:
  #       - loki.example.com
  #      secretName: loki-distributed-tls
  # monitoring:
  #   dashboards:
  #     enabled: true
  #     # -- Alternative namespace to create dashboards ConfigMap in
  #     namespace: null
  #     # -- Additional annotations for the dashboards ConfigMap
  #     annotations: {}
  #     annotations:
  #       grafana_folder: Loki
  #   rules:
  #     enabled: true
  #   serviceMonitor:
  #     enabled: true
  #     metricsInstance:
  #       enabled: true
  #   selfMonitoring:
  #     enabled: false
  #     grafanaAgent:
  #       installOperator: false

  sidecar:
    image:
      repository: ghcr.io/kiwigrid/k8s-sidecar
    rules:
      searchNamespace: ALL
      folder: /rules/fake

  lokiCanary:
    enabled: false

  test:
    enabled: false
  monitoring:
    dashboards:
      enabled: true
      labels:
        grafana_dashboard: "1"

    rules:
      enabled: true
      alerting: true
    serviceMonitor:
      enabled: true
      # labels:
      #   release: kube-prometheus-stack
    podMonitor:
      enabled: true
      # labels:
      #   release: kube-prometheus-stack

      # tableManager is top-level in this chart
tableManager:
  retention_deletes_enabled: true
  retention_period: 168h

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# TEMPO (local storage for dev/POC)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tempo:
  enabled: true
  fullnameOverride: tempo
  serviceMonitor: { enabled: true }
  # replicas: 1
  stream_over_http_enabled: true

  gateway:
    enabled: true
  minio:
    enabled: true
  storage:
    trace:
      backend: s3
      s3:
        access_key: <path:secret/data/minio#postgres_user>
        secret_key: <path:secret/data/minio#postgres_password>
        bucket: "tempo-traces"
        endpoint: "http://minio.minio.svc.cluster.local:9000"
        insecure: true
  traces:
    otlp:
      http:
        enabled: true
      grpc:
        enabled: true
  distributor:
    config:
      log_received_spans:
        enabled: true
      log_discarded_spans:
        enabled: true

  tempo:
    metricsGenerator:
      enabled: true
      remoteWriteUrl: "http://kps-prometheus.monitoring.svc.cluster.local:9090/api/v1/write"
    # overrides:
    #   defaults:
    #     metrics_generator:
    #       processors: [ service-graphs, span-metrics, local-blocks ]

  persistence:
    enabled: true
    size: 20Gi
    storageClassName: nfs-csi

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Promtail (local storage for dev/POC)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
promtail:
  enabled: true
  config:
    logLevel: info
    serverPort: 3101
    clients:
    - url: http://monitoring-stack-loki-gateway.{{ .Release.Namespace }}.svc.cluster.local/loki/api/v1/push
      backoff_config:
        min_period: 500ms
        max_period: 5m
        max_retries: 10
      batchwait: 1s
      batchsize: 1048576 # 1MB
      timeout: 10s

    snippets:
      pipelineStages:
      - cri: {}
      - docker: {}
      - cri: {}
      - match:
          selector: '{app=~".+"} |~ "(error|Error|ERROR)"'
          stages:
          - metrics:
              error_total:
                type: Counter
                description: "Total number of error logs"
                source: log
                config:
                  action: inc
      # - labelallow:
      #   - "namespace"
      #   - "app"
      #   - "pod"
      #   - "host"
      scrapeConfigs: |
        - job_name: kubernetes-pods
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            # Add node name
            - action: replace
              source_labels: [__meta_kubernetes_pod_node_name]
              target_label: node_name
            
            # Add namespace
            - action: replace
              source_labels: [__meta_kubernetes_namespace]
              target_label: namespace
            
            # Add pod name
            - action: replace
              source_labels: [__meta_kubernetes_pod_name]
              target_label: pod
            
            # Add container name
            - action: replace
              source_labels: [__meta_kubernetes_pod_container_name]
              target_label: container
            
            # Add app label if exists
            - action: replace
              source_labels: [__meta_kubernetes_pod_label_app]
              target_label: app
              
            # Add service label if exists
            - action: replace
              source_labels: [__meta_kubernetes_pod_label_service]
              target_label: service
            
            # Set the correct log path - FIXED
            - action: replace
              replacement: /var/log/pods/*$1/*.log
              separator: /
              source_labels: [__meta_kubernetes_pod_uid, __meta_kubernetes_pod_container_name]
              target_label: __path__
          
          pipeline_stages:
            # Handle different log formats
            - match:
                selector: '{stream="stdout"}'
                stages:
                  # Try to parse as JSON first (for structured logs)
                  - json:
                      expressions:
                        log: log
                        message: message
                        level: level
                        timestamp: timestamp
                        time: time
                      drop_malformed: true
                  
                  # If JSON parsing fails, treat as plain text
                  - output:
                      source: log
            
            # Handle stderr separately
            - match:
                selector: '{stream="stderr"}'
                stages:
                  - output:
                      source: message
            
            # Extract log level from unstructured logs
            - regex:
                expression: '(?i)(?P<extracted_level>debug|info|warn|warning|error|fatal|trace)'
                source: message
            
            # Add extracted level as label
            - labels:
                extracted_level:
            
            # Parse timestamps if available
            - match:
                selector: '{extracted_level=~".+"}'
                stages:
                  - timestamp:
                      source: timestamp
                      format: RFC3339Nano
                      fallback_formats:
                        - "2006-01-02 15:04:05"
                        - "2006-01-02T15:04:05Z"
                        - "2006-01-02T15:04:05.999999999Z07:00"
      extraScrapeConfigs: |
        - job_name: journal
          journal:
            path: /var/log/journal
            max_age: 12h
            labels:
              job: systemd-journal
          relabel_configs:
            - source_labels: ['__journal__systemd_unit']
              target_label: 'unit'
            - source_labels: ['__journal__hostname']
              target_label: 'hostname'
            - source_labels: ['__journal__hostname']   # <-- add this
              target_label: 'host'                     # unify label name
        - job_name: custom-config
          pipeline_stages:
          - docker: {}
          - json:
              expressions:
                timestamp: timestamp
                level: level
                thread: thread
                class: logger
                message: message
                context: context
          - labels:
              level:
              class:
              context:
              thread:
          - timestamp:
              format: RFC3339
              source: timestamp
          - output:
              source: message
          kubernetes_sd_configs:
          - role: pod
          relabel_configs:
          - source_labels:
            - __meta_kubernetes_pod_controller_name
            regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?
            action: replace
            target_label: __tmp_controller_name
          - source_labels:
            - __meta_kubernetes_pod_label_app_kubernetes_io_name
            - __meta_kubernetes_pod_label_app
            - __tmp_controller_name
            - __meta_kubernetes_pod_name
            regex: ^;*([^;]+)(;.*)?$
            action: replace
            target_label: app
          - source_labels:
            - __meta_kubernetes_pod_label_app_kubernetes_io_instance
            - __meta_kubernetes_pod_label_release
            regex: ^;*([^;]+)(;.*)?$
            action: replace
            target_label: instance
          - source_labels:
            - __meta_kubernetes_pod_label_app_kubernetes_io_component
            - __meta_kubernetes_pod_label_component
            regex: ^;*([^;]+)(;.*)?$
            action: replace
            target_label: component
          - action: replace
            source_labels:
            - __meta_kubernetes_pod_node_name
            target_label: node_name
          - action: replace
            source_labels:
            - __meta_kubernetes_namespace
            target_label: namespace
          - action: replace
            replacement: $1
            separator: /
            source_labels:
            - namespace
            - app
            target_label: job
          - action: replace
            source_labels:
            - __meta_kubernetes_pod_name
            target_label: pod
          - action: replace
            source_labels:
            - __meta_kubernetes_pod_container_name
            target_label: container
          - action: replace
            replacement: /var/log/pods/*$1/*.log
            separator: /
            source_labels:
            - __meta_kubernetes_pod_uid
            - __meta_kubernetes_pod_container_name
            target_label: __path__
          - action: replace
            regex: true/(.*)
            replacement: /var/log/pods/*$1/*.log
            separator: /
            source_labels:
            - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash
            - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash
            - __meta_kubernetes_pod_container_name
            target_label: __path__
        - job_name: syslog
          syslog:
            listen_address: 0.0.0.0:{{ .Values.extraPorts.syslog.containerPort }}
            label_structured_data: true
            labels:
              job: "syslog"
          relabel_configs:
            - source_labels: ['__syslog_message_hostname']
              target_label: 'host'
            - source_labels: ['__syslog_message_app_name']
              target_label: 'app'

        - job_name: glueops-nginx-with-host
          pipeline_stages:
          - cri: {}
          - json:
              expressions:
                http_host: http_host
          - labels:
              http_host: http_host
          kubernetes_sd_configs:
          - role: pod
          relabel_configs:
          - source_labels:
            - __meta_kubernetes_pod_controller_name
            regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?
            action: replace
            target_label: __tmp_controller_name
          - source_labels:
            - __meta_kubernetes_pod_label_app_kubernetes_io_name
            - __meta_kubernetes_pod_label_app
            - __tmp_controller_name
            - __meta_kubernetes_pod_name
            regex: ^;*([^;]+)(;.*)?$
            action: replace
            target_label: app
          - source_labels:
            - __meta_kubernetes_pod_label_app_kubernetes_io_instance
            - __meta_kubernetes_pod_label_release
            regex: ^;*([^;]+)(;.*)?$
            action: replace
            target_label: instance
          - source_labels:
            - __meta_kubernetes_pod_label_app_kubernetes_io_component
            - __meta_kubernetes_pod_label_component
            regex: ^;*([^;]+)(;.*)?$
            action: replace
            target_label: component
          - action: replace
            source_labels:
            - __meta_kubernetes_pod_node_name
            target_label: node_name
          - action: replace
            source_labels:
            - __meta_kubernetes_namespace
            target_label: namespace
          - action: replace
            replacement: $1
            separator: /
            source_labels:
            - namespace
            - app
            target_label: job
          - action: replace
            source_labels:
            - __meta_kubernetes_pod_name
            target_label: pod
          - action: replace
            source_labels:
            - __meta_kubernetes_pod_container_name
            target_label: container
          - action: replace
            replacement: /var/log/pods/*nginx-controller*/*/*.log
            separator: /
            source_labels:
            - __meta_kubernetes_pod_uid
            - __meta_kubernetes_pod_container_name
            target_label: __path__
          - action: replace
            regex: true/(.*)
            replacement: /var/log/pods/*nginx-controller*/*/*.log
            separator: /
            source_labels:
            - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash
            - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash
            - __meta_kubernetes_pod_container_name
            target_label: __path__
          - action: replace
            target_label: glueops_job_name
            replacement: nginx-with-host
          - action: keep
            source_labels:
            - __meta_kubernetes_pod_name
            regex: ".*nginx-controller.*"
  extraVolumes:
  - name: journal
    hostPath:
      path: /var/log/journal
  - name: audit-logs
    hostPath:
      path: /var/log/audit
  - name: rancher-audit-logs
    hostPath:
      path: /var/log/rancher
  extraVolumeMounts:
  - name: journal
    mountPath: /var/log/journal
    readOnly: true

  - name: audit-logs
    mountPath: /var/log/audit
    readOnly: true

  - name: rancher-audit-logs
    mountPath: /var/log/rancher
    readOnly: true
  serviceMonitor:
    enabled: true
  extraPorts:
    syslog:
      name: tcp-syslog
      containerPort: 1514
      protocol: TCP
      service:
        type: ClusterIP
        port: 1514
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Alloy(local storage for dev/POC)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
alloy:
  alloy:
    mounts:
      varlog: true

    # Service annotations for Prometheus scraping
    service:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "12345"
        prometheus.io/path: "/metrics"

    # Additional ports for OTLP
    extraPorts:
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP

    configMap:
      content: |
        // OTLP receiver configuration
        otelcol.receiver.otlp "default" {
          grpc {
            endpoint = "0.0.0.0:4317"
          }
          http {
            endpoint = "0.0.0.0:4318"
          }
          output {
            metrics = [otelcol.exporter.prometheus.default.input]
            traces  = [otelcol.exporter.otlp.tempo.input]
            logs    = [otelcol.exporter.loki.default.input]
          }
        }

        // Metrics to Prometheus
        otelcol.exporter.prometheus "default" {
          forward_to = [prometheus.remote_write.default.receiver]
        }

        prometheus.remote_write "default" {
          endpoint {
            url = "http://kps-prometheus.monitoring.svc.cluster.local/api/v1/write"
          }
        }

        // Traces to Tempo
        otelcol.exporter.otlp "tempo" {
          client {
            endpoint = "http://tempo.monitoring.svc.cluster.local:4317"
            tls {
              insecure = true
            }
          }
        }

        // Logs to Loki
        otelcol.exporter.loki "default" {
          forward_to = [loki.write.endpoint.receiver]
        }

        loki.write "endpoint" {
          endpoint {
              url = "http://monitoring-stack-loki-gateway.monitoring.svc.cluster.local/loki/api/v1/push"

              // Enhanced batching for better performance
              batch_size = "1MB"
              batch_wait = "1s"

          }
        }
