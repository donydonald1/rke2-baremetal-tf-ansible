---
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: {{ .Values.liveCluster }}
  # namespace: default
  annotations:
    kyverno.io/ignore: "true"
spec:
  # using only 1 replica: very difficult to drain the node where postgres is running
  # using more than 1 replica: write amplification issues when leveraging replicated storage (e.g. ceph)
  description: "PostGreSQL cluster managed by CloudNative-PG Operator"
  instances: {{ .Values.replicas }}
  imageName: ghcr.io/cloudnative-pg/postgis:17-3.5
  primaryUpdateStrategy: unsupervised
  startDelay: 7200
  storage:
    size: {{ .Values.storage.size }}
    storageClass:  {{ .Values.storage.storageClass }}
  walStorage:
    storageClass: {{ .Values.storage.storageClass }}
    size: {{ .Values.storage.size }}
  enableSuperuserAccess: true
  superuserSecret: 
    name: {{ .Values.superuserSecret }}
  bootstrap:
    # recovery:
    #   source: {{ .Values.previousCluster }}
    initdb:
      localeCollate: 'en_US.utf8'
      localeCType: 'en_US.utf8'
      dataChecksums: true
      database: grafana
      owner: grafana
      secret:
        name: {{ .Values.superuserSecret }}
      postInitTemplateSQL:
        - CREATE EXTENSION postgis;
        - CREATE EXTENSION postgis_topology;
        - CREATE EXTENSION fuzzystrmatch;
        - CREATE EXTENSION postgis_tiger_geocoder;
  managed:
    roles:
    - name: grafana
      ensure: present
      comment: "friendly user"
      login: true
      superuser: true
      createdb: true
      createrole: true
      passwordSecret:
        name: {{ .Values.superuserSecret }}
      inRoles:
      - pg_monitor
      - pg_signal_backend
  postgresql:
    parameters:
      max_wal_size: "2GB"
      min_wal_size: "1280MB"
      max_wal_senders: "16"
      wal_keep_size: "512MB"
      # wal_level: "replica" # "logical" # immutable
      fsync: "on"
      # hot_standby: "on"   #  immutable
      log_connections: "false"
      log_disconnections: "false"
      log_hostname: "false"
      checkpoint_timeout: "15min" # Can be higher (15min)
      max_connections: "1000"
      shared_buffers: 512MB
  monitoring:
    enablePodMonitor: true
  # https://github.com/cloudnative-pg/cloudnative-pg/issues/2570
  enablePDB: false
  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "2Gi"
  backup:
    retentionPolicy: 15d
    barmanObjectStore:
      wal:
        compression: bzip2
        maxParallel: 8
      destinationPath: s3://postgres/
      endpointURL: http://minio.minio.svc.cluster.local:9000
      serverName: {{ .Values.liveCluster }}
      s3Credentials:
        accessKeyId:
          name: postgres-minio-secret
          key: MINIO_ACCESS_KEY
        secretAccessKey:
          name: postgres-minio-secret
          key: MINIO_SECRET_KEY

    # use to migrate from an existing cnpg cluster to a new cluster
    # initdb:
    #   import:
    #     type: monolith
    #     databases:
    #       - "*"
    #     roles:
    #       - "*"
    #     source:
    #       externalCluster: dendrite-psql-v16
  externalClusters:
    # this represents the s3 backup to restore from. *nota-bene: the backup must be the same major version of the target cluster
    - name: {{ .Values.previousCluster }}
      barmanObjectStore:
        wal:
          compression: bzip2
          maxParallel: 8
        destinationPath: s3://postgres/
        endpointURL: http://minio.minio.svc.cluster.local:9000
        serverName: {{ .Values.previousCluster }}
        s3Credentials:
          accessKeyId:
            name: postgres-minio-secret
            key: MINIO_ACCESS_KEY
          secretAccessKey:
            name: postgres-minio-secret
            key: MINIO_SECRET_KEY
